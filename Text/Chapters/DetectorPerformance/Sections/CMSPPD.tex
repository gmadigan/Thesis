
%Physics Performance and Datasets (PPD) is a CMS-wide coordination area responsible for the centralized software, called CMSSW, used to process all data originating from CMS. The mandate of the PPD group is to develop the necessary code and validate the physics performance of data processed with CMSSW in event generation, physics and detector simulation, reconstruction, detector alignment and calibration, data quality monitoring, analysis and statistical tools, and trigger validation. The management and execution of each task within PPD is performed by a dedicated subgroup. 

%CMS Data Quality Monitoring (DQM) is divided into two streams: online DQM, where any problems during CMS detector operation are identified, and offline DQM combined with Data Certification (DC), for establishing if data are suitable for physics analysis. DQM is performed by experts within each subsystem, but a subgroup within PPD, the DQM-DC group, organizes the centralization of the monitoring modules in CMSSW, operates the live monitoring applications and visualization tool (DQM GUI), organizes the DQM shifts of each subsytem, and coordinates the expert DC and publishes the list of ``good'' data. 

%\subsection{CMSSW}
%\label{sec:CMSSW}

%CMSSW is the full collection of software modules used in processing CMS data. Development of CMSSW code by the different detector subsystems, coordination areas, and physics groups, gets included in scheduled production and pre-production releases, with approval going through the PPD group using pre-production physics validation samples.

\subsection{Data formats and quality monitoring}
\label{sec:DataFormats}

The CMS experiment stores recorded and simulated collision data in several file formats with differing size, compression, structure, etc., categorized into tiers. As data undergo successive processing, they are refined from detector-level hits all the way to high-level physics objects, with each stage being supported by a different data tier. The lowest-level data format is DAQ-RAW, which comes directly from the front-end electronics with an L1A. These DAQ-RAW files feed the online HLT algorithms, and after online formatting, events are stored in the RAW data tier. RAW files contain detector data, L1 trigger result, HLT trigger bits, and a few high-level quantities needed for HLT decisions, and range in size from 0.70-0.75 MB per event. Simulated events generated with Monte Carlo (MC) are stored as GEN files. Particle interactions with detector components are realized as energy depositions, also called sim hits, stored as SIM data. The detector response to sim hits are stored in the DIGI format at 1.5 MB per event. DIGI files are effectively the same format as RAW data from the detector. Event reconstruction at Tier-0 of RAW and DIGI data builds complex physics objects like tracks, verticies, jets, etc. and reconstructed hits and clusters, and stores the output in the RECO format. Events in RECO contain more sophisticated data than in RAW, resulting in a substantially larger event size of 1.3-1.4 MB per event. Physics analyses during Run I relied on centrally-stored datasets in a special format called Analysis Object Data (AOD). AOD files are much smaller than RECO, making them easier to store and faster to process, by compressing each event down to a mere 0.05 MB. From these AOD datasets, analyses could generate their own ntuples with analysis-specific data according to their needs. During Run II, more size-efficient versions of AOD were released, called Mini-AOD and Nano-AOD, which are both on the order of 1 KB per event.

CMS Data Quality Monitoring (DQM) is divided into two streams: online DQM, where any problems during CMS detector operation are identified, and offline DQM combined with Data Certification (DC), for establishing if data are suitable for physics analysis. DQM is performed by experts within each subsystem, while the DQM-DC subgroup of Physics Performance and Datasets organizes the centralization of the monitoring modules in CMSSW, operates the live monitoring applications and visualization tool (DQM GUI), organizes the DQM shifts of each subsytem, coordinates the experts to certify data, and publishes the list of good data for phsyics analysis.

\subsection{Alignment, Calibration, and Database}
\label{sec:AlCaDB}

Physics Performance and Datasets (PPD) is a CMS-wide coordination area responsible for the centralized software, called CMSSW, used to process all data originating from CMS. CMSSW is the full collection of software modules used in processing CMS data. Development of CMSSW code by the different detector subsystems, coordination areas, and physics groups, gets included in scheduled production and pre-production releases, with approval going through the PPD group using pre-production physics validation samples. The mandate of the PPD group is to develop the necessary code and validate the physics performance of data processed with CMSSW in event generation, physics and detector simulation, reconstruction, detector alignment and calibration, data quality monitoring, analysis and statistical tools, and trigger validation. The management and execution of each task within PPD is performed by a dedicated subgroup. Inside PPD, the Alignment, Calibration, and Database (AlCaDB) group oversee the storage, application, and monitoring of conditions data supplied by the CMS subsystems for the alignment and calibration of the detector. The AlCaDB group comprises a core team of experts, plus at least one contact from each Detector Performance Group, Physics Object Group, and the Trigger Studies Group. Contacts act as representatives of their respective DPG/POG/TSG, communicating news, activity, and requests from the AlCaDB core team, and are responsible for subsystem/physics/trigger-dependant activity within AlCaDB.

\subsubsection{Detector alignment and calibration}

Inevitable misalignment of the CMS detector caused by by tolerances, static fields (magnetic, gravitational), and temperature fluctuations, presents an incredible challenge for offline reconstruction. In particular, the fine-grain resolution of the tracking technologies---pixel, strip, and muon---requires precision alignment of the many independent sensors to fully exploit. To accurately model the true detector geometry in reconstruction, a set of alignment corrections to the ideal geometry must be measured by each subsystem. The tools used to extract the alignment corrections vary accross the subsytems, from purpose-built optical sensors to data-derrived studies.

The signals read out by the electronics of each CMS subsystem require some degree of calibration to accurately interpret. A set of parameters for the signals in each channel, like gain, pedestal, arrival time, etc., are extracted by controlled measurements. These calibration constants are used in reconstruction to normalize signals in real data-taking or model real signals in simulation, although the exact applications vary significantly for each subdetector.

\subsubsection{Conditions data}

Conditions data is a CMS Collaboration term that encompasses all detector-related information not contained in RAW data required by the online (HLT) and Offline processing streams that might be time-, run-, or event-dependent, or cannot be easily hard-coded. For example, the HLT runs on a self-contained binary executible, meaning any dynamic information required by the HLT algorithms must be accessed through an external source. In Offline processing, it is simply more convinient to retrieve certain parameters that may frequently change rather than hard-coding them in software. Simulated data likewise use conditions data, where modeling the detector-response should reflect the real state of the CMS detector during a given data-taking period. Examples of conditions data are calibration constants, alignment corrections, dead channels, cable mapping, geometry, etc., although the exact conditions are highly detector-specific. All conditions data are uploaded on an as-needed basis to a central SQLite database where they can be read out 

A collection of conditions data is uploaded and stored in the conditions database as a Payload (identified with a hash), along with a collection of metadata. Each Payload represents a set of conditions data covering a specific time period of data-taking, specified by a metadata parameter called the Interval Of Validitiy (IOV). An IOV is the run number from which the conditions data are valid; a range of time defined by a lower bound and extending to infinity. Payloads are accessed in every physics data processing workflow through a Record, representing each ``type'' of conditions data, that imports the database content. Another metadata parameter is a label called a Tag that identifies a set of Payload and IOV together. The insertion time defines the time of each Payload's creation in the database. The last piece of metadata is a Global Tag (GT): a label identifying the set of Tags associated to each Record used in a given workflow. For every CMSSW release, different production workflows are configured to retrieve conditions data under a specific GTs that address different data processing, e.g., HLT, data reconstruction, MC GEN-SIM, MC DIGI-RECO, etc. For this reason GTs, as well as Tags, are always associated with specific data reconstruction scenarios: Online (Express, Promt, and HLT), Offline, and MC. A GT is always prepared by the AlCaDB experts, while contacts are responsible for either approving existing Tags currently in a GT or uploading new Tags.

